{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 7. 앙상블 학습과 랜덤 포레스트"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 앙상블 방법 <sup>ensemble method</sup>\n",
    "* 앙상블 학습 알고리즘\n",
    "* 대중의 지혜와 비슷하게 일련의 예측기(즉, 분류나 회귀 모델)로부터 예측을 수집하면 가장 좋은 모델 하나보다 더 좋은 예측을 얻을 수 있다.\n",
    "* 일련의 예측기를 **앙상블**부르기 때문에, 이를 **앙상블 학습**이라고 한다.\n",
    "* 훈련 세트로부터 무작위로 각기 다른 서브셋을 만들어 일련의 결정 트리 분류기를 훈련시킬 수 있다. 모든 개별 트리의 예측을 구하고 그런 다음 가장 많은 선택을 받은 클래스를 예측으로 삼는다.\n",
    "\n",
    "### 랜덤 포레스트 <sup>ramdom forest</sup>\n",
    "* 결정 트리의 앙상블\n",
    "* 간단한 방법임에도 오늘날 강력한 머신러닝 알고리즘 중 하나"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1 투표 기반 분류기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다수결 투표로 정해지는 분류기를 **직접 투표**<sup>hard voting</sup> 분류기라고 한다.\n",
    "\n",
    "큰 수의 법칙<sup>law of large numbers</sup>에 따라 각 분류기가 약한 학습기<sup>weak learner</sup> (즉, 랜덤 추측보다 조금 더 높은 성능을 내는 분류기) 일지라도 충분하게 많고 다양하다면 앙상블은 (높은 정확도를 내는) 강한 학습기<sup>strong learner</sup> 가 될 수 있다.\n",
    "\n",
    "앙상블 방법은 예측기가 가능한 한 서로 독립적일 때 최고의 성능을 발휘한다. 다양한 분류기를 얻는 한 가지 방법은 각기 다른 알고리즘으로 학습시키는 것이다. 이렇게 하면 매우 다른 종류의 오차를 만들 가능성이 높기 때문에 앙상블 모델의 정확도를 향상시킨다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VotingClassifier(estimators=[('lr', LogisticRegression()),\n",
       "                             ('rf', RandomForestClassifier()), ('svc', SVC())])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_moons\n",
    "\n",
    "X, y = make_moons(n_samples=500, noise=0.30, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "\n",
    "log_clf = LogisticRegression()\n",
    "rnd_clf = RandomForestClassifier()\n",
    "svm_clf = SVC()\n",
    "\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators = [('lr', log_clf), ('rf', rnd_clf), ('svc', svm_clf)],\n",
    "    voting = 'hard'\n",
    ")\n",
    "\n",
    "voting_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression 0.864\n",
      "RandomForestClassifier 0.888\n",
      "SVC 0.896\n",
      "VotingClassifier 0.904\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "for clf in (log_clf, rnd_clf, svm_clf, voting_clf):\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(clf.__class__.__name__, accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 간접 투표 <sup>soft voting</sup>\n",
    "* 모든 분류기가 클래스의 확률을 예측할 수 있으면 (즉, predict_proba() 메서드가 있으면) 개별 분류기의 예측을 평균 내어 확률이 가장 높은 클래스를 예측할 수 있다.\n",
    "* 확률이 높은 투표에 비중을 더 두기 때문에 직접 투표 방식보다 성능이 높다.\n",
    "* voting = \"soft\" 옵션을 사용하고 모든 분류기가 클래스의 확률을 추정할 수 있어야 한다.\n",
    " * SVG는 기본값으로 클래스 확률을 제공하지 않으ㅡ므로 probability 매개변수를 True로 지정해야 한다. (이때 교차 검증을 사용하므로 훈련 속도가 느려진다) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VotingClassifier(estimators=[('lr', LogisticRegression()),\n",
       "                             ('rf', RandomForestClassifier()),\n",
       "                             ('svc', SVC(probability=True))],\n",
       "                 voting='soft')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_clf = LogisticRegression()\n",
    "rnd_clf = RandomForestClassifier()\n",
    "svm_clf = SVC(probability = True)\n",
    "\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators = [('lr', log_clf), ('rf', rnd_clf), ('svc', svm_clf)],\n",
    "    voting = 'soft'\n",
    ")\n",
    "\n",
    "voting_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression 0.864\n",
      "RandomForestClassifier 0.896\n",
      "SVC 0.896\n",
      "VotingClassifier 0.904\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "for clf in (log_clf, rnd_clf, svm_clf, voting_clf):\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(clf.__class__.__name__, accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2 배깅과 페이스팅"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다양한 분류기를 만드는 또 다른 방법은 같은 알고리즘을 사용하고 훈련 세트의 서브셋을 무작위로 구성하여 분류기를 각기 다르게 학습시키는 것이다.\n",
    "\n",
    "* 배깅<sup>bagging</sup>\n",
    " * 훈련 세트에서 중복을 허용하여 샘플링하는 방식\n",
    "* 페이스팅<sup>pasting</sup>\n",
    " * 중복을 허용하지 않고 샘플링하는 방식\n",
    " \n",
    "수집 함수는 전형적으로 분류일 때 **통계적 최빈값**<sup>statistical mode</sup> (즉, 직접 투표 분류기처럼 가장 많은 예측 결과)이고 회귀에 대해서는 평균을 계산한다.\n",
    "\n",
    "개별 예측기는 크게 편향되더라도 수집 함수를 통과하면 편향과 분산이 모두 감소한다. 일반적으로 앙상블 결과는 원본 데이터셋으로 하나의 예측기를 훈련시킬 때와 비교해 편향은 비슷하지만 분산은 줄어든다.\n",
    "\n",
    "예측기는 모두 동시에 다른 CPU 코어나 서버에서 병렬로 학습시킬 수 있다. 이와 유사하게 예측도 병렬로 수행할 수 있다. (확장성)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2.1 사이킷런의 배깅과 페이스팅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 회귀는 BaggingRegressor API 사용\n",
    "# BaggingClassifier는 기반이 되는 분류기가 클래스 확률을 추정할 수 있으면 직접 투표 대신 자동으로 간접 투표 방식을 사용한다\n",
    "from sklearn.ensemble import BaggingClassifier \n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "bag_clf = BaggingClassifier(\n",
    "    DecisionTreeClassifier(), # 결정 트리 분류기 사용\n",
    "    n_estimators = 500, # 분류기 500개\n",
    "    max_samples = 100, # 무작위로 선택된 100개의 샘플\n",
    "    bootstrap = True, # 배깅은 True, 페이스팅은 False\n",
    "    n_jobs = -1 # 훈련과 예측에 사용할 CPU 코어 수. -1은 가용한 모든 코어 사용\n",
    ")\n",
    "bag_clf.fit(X_train, y_train)\n",
    "y_pred = bag_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "부트스트래핑은 각 예측기가 학습하는 서브셋에 다양성을 증가시키므로 배깅이 페이스팅보다 편향이 조금 높다. 하지만 다양성은 예측기들의 상관 관계를 줄이므로 앙상블의 분산을 감소시킨다. 전반적으로 배깅이 더 나은 모델을 만들기 때문에 일반적으로 더 선호하는 경향이 있다.\n",
    "\n",
    "시간과 CPU 파워에 여유가 있다면 교차 검증으로 배깅과 페이스팅을 모두 평가하여 더 나은쪽을 선택하는 것이 좋다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2.2 oob 평가"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "배깅을 사용하면 어떤 샘플은 한 예측기를 위해 여러 번 샘플링되고 어떤 것은 전혀 선택되지 않을 수 있다. BaggingClassifier는 평균적으로 각 예측기에 63% 정도만 샘플링된다. 이때 선택되지 않은 훈련 샘플의 나머지 37%를 oob<sup>out-of-bag</sup> 샘플이라고 부른다. 예측기마다 oob 샘플은 모두 다르다.\n",
    "\n",
    "앙상블의 평가는 별도의 검증 세트 필요 없이 각 예측기의 oob 평가를 평균하여 얻는다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.92"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_clf = BaggingClassifier(\n",
    "    DecisionTreeClassifier(),\n",
    "    n_estimators = 500,\n",
    "    max_samples = 100,\n",
    "    bootstrap = True,\n",
    "    n_jobs = -1,\n",
    "    oob_score = True # 훈련이 끝난 후 자동으로 oob 평가를 수행\n",
    ")\n",
    "bag_clf.fit(X_train, y_train)\n",
    "bag_clf.oob_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.928"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_pred = bag_clf.predict(X_test)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.37817259, 0.62182741],\n",
       "       [0.42819843, 0.57180157],\n",
       "       [1.        , 0.        ],\n",
       "       [0.0125    , 0.9875    ],\n",
       "       [0.02486188, 0.97513812],\n",
       "       [0.0959596 , 0.9040404 ],\n",
       "       [0.38441558, 0.61558442],\n",
       "       [0.06994819, 0.93005181],\n",
       "       [0.94910941, 0.05089059],\n",
       "       [0.81233933, 0.18766067],\n",
       "       [0.56153846, 0.43846154],\n",
       "       [0.04065041, 0.95934959],\n",
       "       [0.78552279, 0.21447721],\n",
       "       [0.8680203 , 0.1319797 ],\n",
       "       [0.88502674, 0.11497326],\n",
       "       [0.1023622 , 0.8976378 ],\n",
       "       [0.05333333, 0.94666667],\n",
       "       [0.92245989, 0.07754011],\n",
       "       [0.66052632, 0.33947368],\n",
       "       [0.96514745, 0.03485255],\n",
       "       [0.05066667, 0.94933333],\n",
       "       [0.19791667, 0.80208333],\n",
       "       [0.90414508, 0.09585492],\n",
       "       [0.9870801 , 0.0129199 ],\n",
       "       [0.96850394, 0.03149606],\n",
       "       [0.00268097, 0.99731903],\n",
       "       [0.95739348, 0.04260652],\n",
       "       [1.        , 0.        ],\n",
       "       [0.03684211, 0.96315789],\n",
       "       [0.73536896, 0.26463104],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.00793651, 0.99206349],\n",
       "       [0.07629428, 0.92370572],\n",
       "       [0.11671088, 0.88328912],\n",
       "       [0.98453608, 0.01546392],\n",
       "       [0.01558442, 0.98441558],\n",
       "       [0.60206718, 0.39793282],\n",
       "       [0.01234568, 0.98765432],\n",
       "       [0.99737533, 0.00262467],\n",
       "       [0.12919897, 0.87080103],\n",
       "       [0.36623377, 0.63376623],\n",
       "       [0.98963731, 0.01036269],\n",
       "       [0.97905759, 0.02094241],\n",
       "       [0.01015228, 0.98984772],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.06020942, 0.93979058],\n",
       "       [0.96791444, 0.03208556],\n",
       "       [0.04102564, 0.95897436],\n",
       "       [0.95214106, 0.04785894],\n",
       "       [0.84375   , 0.15625   ],\n",
       "       [0.92063492, 0.07936508],\n",
       "       [0.81868132, 0.18131868],\n",
       "       [0.00769231, 0.99230769],\n",
       "       [0.06824147, 0.93175853],\n",
       "       [0.80208333, 0.19791667],\n",
       "       [0.02255639, 0.97744361],\n",
       "       [0.0166205 , 0.9833795 ],\n",
       "       [0.07277628, 0.92722372],\n",
       "       [0.84863524, 0.15136476],\n",
       "       [0.62760417, 0.37239583],\n",
       "       [0.73015873, 0.26984127],\n",
       "       [0.9895288 , 0.0104712 ],\n",
       "       [0.04359673, 0.95640327],\n",
       "       [0.76844784, 0.23155216],\n",
       "       [0.97097625, 0.02902375],\n",
       "       [0.98969072, 0.01030928],\n",
       "       [0.58465608, 0.41534392],\n",
       "       [0.98092643, 0.01907357],\n",
       "       [0.31550802, 0.68449198],\n",
       "       [0.33160622, 0.66839378],\n",
       "       [0.42227979, 0.57772021],\n",
       "       [0.65796345, 0.34203655],\n",
       "       [0.00995025, 0.99004975],\n",
       "       [0.33673469, 0.66326531],\n",
       "       [0.86075949, 0.13924051],\n",
       "       [1.        , 0.        ],\n",
       "       [0.04787234, 0.95212766],\n",
       "       [0.96782842, 0.03217158],\n",
       "       [0.00263158, 0.99736842],\n",
       "       [0.23262032, 0.76737968],\n",
       "       [0.11702128, 0.88297872],\n",
       "       [0.46858639, 0.53141361],\n",
       "       [0.99477807, 0.00522193],\n",
       "       [0.05181347, 0.94818653],\n",
       "       [0.61417323, 0.38582677],\n",
       "       [0.05181347, 0.94818653],\n",
       "       [0.03183024, 0.96816976],\n",
       "       [0.00268097, 0.99731903],\n",
       "       [0.36631016, 0.63368984],\n",
       "       [1.        , 0.        ],\n",
       "       [0.00526316, 0.99473684],\n",
       "       [0.06217617, 0.93782383],\n",
       "       [0.01351351, 0.98648649],\n",
       "       [0.77545692, 0.22454308],\n",
       "       [0.65013055, 0.34986945],\n",
       "       [0.05026455, 0.94973545],\n",
       "       [1.        , 0.        ],\n",
       "       [0.34545455, 0.65454545],\n",
       "       [0.70789474, 0.29210526],\n",
       "       [0.00779221, 0.99220779],\n",
       "       [0.08311688, 0.91688312],\n",
       "       [0.47297297, 0.52702703],\n",
       "       [0.95844156, 0.04155844],\n",
       "       [0.04404145, 0.95595855],\n",
       "       [0.97709924, 0.02290076],\n",
       "       [0.4591029 , 0.5408971 ],\n",
       "       [0.24802111, 0.75197889],\n",
       "       [0.99246231, 0.00753769],\n",
       "       [0.23157895, 0.76842105],\n",
       "       [0.83497537, 0.16502463],\n",
       "       [0.24365482, 0.75634518],\n",
       "       [0.77749361, 0.22250639],\n",
       "       [0.9947644 , 0.0052356 ],\n",
       "       [0.99491094, 0.00508906],\n",
       "       [0.        , 1.        ],\n",
       "       [0.0025641 , 0.9974359 ],\n",
       "       [0.45931759, 0.54068241],\n",
       "       [1.        , 0.        ],\n",
       "       [0.0610687 , 0.9389313 ],\n",
       "       [0.98503741, 0.01496259],\n",
       "       [0.97468354, 0.02531646],\n",
       "       [1.        , 0.        ],\n",
       "       [0.93800539, 0.06199461],\n",
       "       [0.96587927, 0.03412073],\n",
       "       [0.04485488, 0.95514512],\n",
       "       [0.93530997, 0.06469003],\n",
       "       [0.95103093, 0.04896907],\n",
       "       [0.02356021, 0.97643979],\n",
       "       [0.21774194, 0.78225806],\n",
       "       [0.875     , 0.125     ],\n",
       "       [0.43733333, 0.56266667],\n",
       "       [0.90078329, 0.09921671],\n",
       "       [0.00262467, 0.99737533],\n",
       "       [0.02864583, 0.97135417],\n",
       "       [0.81132075, 0.18867925],\n",
       "       [0.76203209, 0.23796791],\n",
       "       [0.58762887, 0.41237113],\n",
       "       [0.87341772, 0.12658228],\n",
       "       [0.96354167, 0.03645833],\n",
       "       [0.11306533, 0.88693467],\n",
       "       [0.81748072, 0.18251928],\n",
       "       [0.03125   , 0.96875   ],\n",
       "       [0.00506329, 0.99493671],\n",
       "       [0.13506494, 0.86493506],\n",
       "       [0.74805195, 0.25194805],\n",
       "       [0.97208122, 0.02791878],\n",
       "       [1.        , 0.        ],\n",
       "       [0.05277045, 0.94722955],\n",
       "       [0.00253165, 0.99746835],\n",
       "       [0.06878307, 0.93121693],\n",
       "       [0.03157895, 0.96842105],\n",
       "       [0.98947368, 0.01052632],\n",
       "       [0.99236641, 0.00763359],\n",
       "       [0.88601036, 0.11398964],\n",
       "       [0.99746193, 0.00253807],\n",
       "       [1.        , 0.        ],\n",
       "       [0.87435897, 0.12564103],\n",
       "       [0.01025641, 0.98974359],\n",
       "       [0.62253521, 0.37746479],\n",
       "       [0.32808399, 0.67191601],\n",
       "       [0.0596206 , 0.9403794 ],\n",
       "       [0.01612903, 0.98387097],\n",
       "       [0.37275064, 0.62724936],\n",
       "       [0.99734043, 0.00265957],\n",
       "       [0.96391753, 0.03608247],\n",
       "       [0.        , 1.        ],\n",
       "       [0.97943445, 0.02056555],\n",
       "       [0.04497354, 0.95502646],\n",
       "       [0.00262467, 0.99737533],\n",
       "       [0.93384224, 0.06615776],\n",
       "       [0.01511335, 0.98488665],\n",
       "       [0.00817439, 0.99182561],\n",
       "       [1.        , 0.        ],\n",
       "       [0.03733333, 0.96266667],\n",
       "       [0.84793814, 0.15206186],\n",
       "       [0.93733681, 0.06266319],\n",
       "       [0.06203474, 0.93796526],\n",
       "       [0.96134021, 0.03865979],\n",
       "       [0.94148936, 0.05851064],\n",
       "       [0.98254364, 0.01745636],\n",
       "       [0.00518135, 0.99481865],\n",
       "       [0.01092896, 0.98907104],\n",
       "       [0.99731183, 0.00268817],\n",
       "       [0.2455243 , 0.7544757 ],\n",
       "       [0.98488665, 0.01511335],\n",
       "       [0.12244898, 0.87755102],\n",
       "       [0.03412073, 0.96587927],\n",
       "       [0.98974359, 0.01025641],\n",
       "       [0.        , 1.        ],\n",
       "       [0.20923913, 0.79076087],\n",
       "       [0.89405685, 0.10594315],\n",
       "       [0.92287918, 0.07712082],\n",
       "       [0.616     , 0.384     ],\n",
       "       [0.66323907, 0.33676093],\n",
       "       [0.024     , 0.976     ],\n",
       "       [0.25129534, 0.74870466],\n",
       "       [0.9972752 , 0.0027248 ],\n",
       "       [0.9119171 , 0.0880829 ],\n",
       "       [0.92574257, 0.07425743],\n",
       "       [0.98108108, 0.01891892],\n",
       "       [0.02702703, 0.97297297],\n",
       "       [0.01312336, 0.98687664],\n",
       "       [0.08648649, 0.91351351],\n",
       "       [0.47074468, 0.52925532],\n",
       "       [0.        , 1.        ],\n",
       "       [0.03385417, 0.96614583],\n",
       "       [0.96446701, 0.03553299],\n",
       "       [0.10422535, 0.89577465],\n",
       "       [0.09549072, 0.90450928],\n",
       "       [0.88324873, 0.11675127],\n",
       "       [0.06459948, 0.93540052],\n",
       "       [0.39534884, 0.60465116],\n",
       "       [0.00511509, 0.99488491],\n",
       "       [1.        , 0.        ],\n",
       "       [0.01007557, 0.98992443],\n",
       "       [0.00261097, 0.99738903],\n",
       "       [0.91709845, 0.08290155],\n",
       "       [0.89672544, 0.10327456],\n",
       "       [0.94791667, 0.05208333],\n",
       "       [0.01530612, 0.98469388],\n",
       "       [0.07526882, 0.92473118],\n",
       "       [0.94179894, 0.05820106],\n",
       "       [0.13131313, 0.86868687],\n",
       "       [0.0080429 , 0.9919571 ],\n",
       "       [0.27968338, 0.72031662],\n",
       "       [0.97127937, 0.02872063],\n",
       "       [0.81914894, 0.18085106],\n",
       "       [0.09487179, 0.90512821],\n",
       "       [0.72631579, 0.27368421],\n",
       "       [0.94987469, 0.05012531],\n",
       "       [0.13527851, 0.86472149],\n",
       "       [0.13684211, 0.86315789],\n",
       "       [0.99480519, 0.00519481],\n",
       "       [0.        , 1.        ],\n",
       "       [0.01075269, 0.98924731],\n",
       "       [0.01612903, 0.98387097],\n",
       "       [0.34464752, 0.65535248],\n",
       "       [0.86410256, 0.13589744],\n",
       "       [0.05483029, 0.94516971],\n",
       "       [0.98961039, 0.01038961],\n",
       "       [0.85714286, 0.14285714],\n",
       "       [0.0026178 , 0.9973822 ],\n",
       "       [0.75641026, 0.24358974],\n",
       "       [0.9947644 , 0.0052356 ],\n",
       "       [0.0132626 , 0.9867374 ],\n",
       "       [0.98969072, 0.01030928],\n",
       "       [0.06565657, 0.93434343],\n",
       "       [0.0025641 , 0.9974359 ],\n",
       "       [0.10443864, 0.89556136],\n",
       "       [0.25260417, 0.74739583],\n",
       "       [0.8537234 , 0.1462766 ],\n",
       "       [0.06718346, 0.93281654],\n",
       "       [0.99498747, 0.00501253],\n",
       "       [0.68041237, 0.31958763],\n",
       "       [0.08732394, 0.91267606],\n",
       "       [0.6744186 , 0.3255814 ],\n",
       "       [0.8328841 , 0.1671159 ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.9947644 , 0.0052356 ],\n",
       "       [0.04134367, 0.95865633],\n",
       "       [0.        , 1.        ],\n",
       "       [0.73726542, 0.26273458],\n",
       "       [0.        , 1.        ],\n",
       "       [0.98950131, 0.01049869],\n",
       "       [0.15466667, 0.84533333],\n",
       "       [0.76785714, 0.23214286],\n",
       "       [0.11948052, 0.88051948],\n",
       "       [0.99240506, 0.00759494],\n",
       "       [0.87817259, 0.12182741],\n",
       "       [0.00529101, 0.99470899],\n",
       "       [0.09392265, 0.90607735],\n",
       "       [0.14814815, 0.85185185],\n",
       "       [0.07853403, 0.92146597],\n",
       "       [0.00260417, 0.99739583],\n",
       "       [0.97527473, 0.02472527],\n",
       "       [0.84334204, 0.15665796],\n",
       "       [0.17929293, 0.82070707],\n",
       "       [0.93066667, 0.06933333],\n",
       "       [0.02105263, 0.97894737],\n",
       "       [0.66307278, 0.33692722],\n",
       "       [0.11351351, 0.88648649],\n",
       "       [0.95710456, 0.04289544],\n",
       "       [0.89230769, 0.10769231],\n",
       "       [0.00273973, 0.99726027],\n",
       "       [0.92564103, 0.07435897],\n",
       "       [0.89473684, 0.10526316],\n",
       "       [0.0075    , 0.9925    ],\n",
       "       [0.0604534 , 0.9395466 ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.04675325, 0.95324675],\n",
       "       [0.98976982, 0.01023018],\n",
       "       [0.06426735, 0.93573265],\n",
       "       [0.92838875, 0.07161125],\n",
       "       [0.99470899, 0.00529101],\n",
       "       [0.00255754, 0.99744246],\n",
       "       [0.03466667, 0.96533333],\n",
       "       [0.68834688, 0.31165312],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.62468514, 0.37531486],\n",
       "       [0.83557951, 0.16442049],\n",
       "       [0.99206349, 0.00793651],\n",
       "       [0.69190601, 0.30809399],\n",
       "       [0.5       , 0.5       ],\n",
       "       [0.02105263, 0.97894737],\n",
       "       [0.83033419, 0.16966581],\n",
       "       [0.01023018, 0.98976982],\n",
       "       [1.        , 0.        ],\n",
       "       [0.7191601 , 0.2808399 ],\n",
       "       [0.98992443, 0.01007557],\n",
       "       [1.        , 0.        ],\n",
       "       [0.83018868, 0.16981132],\n",
       "       [0.28756477, 0.71243523],\n",
       "       [0.12628866, 0.87371134],\n",
       "       [0.16622691, 0.83377309],\n",
       "       [0.00261097, 0.99738903],\n",
       "       [0.72105263, 0.27894737],\n",
       "       [0.9012987 , 0.0987013 ],\n",
       "       [0.04040404, 0.95959596],\n",
       "       [0.9974359 , 0.0025641 ],\n",
       "       [0.95897436, 0.04102564],\n",
       "       [0.98989899, 0.01010101],\n",
       "       [0.00791557, 0.99208443],\n",
       "       [0.0437018 , 0.9562982 ],\n",
       "       [0.91891892, 0.08108108],\n",
       "       [0.92105263, 0.07894737],\n",
       "       [0.99733333, 0.00266667],\n",
       "       [0.19685039, 0.80314961],\n",
       "       [0.98780488, 0.01219512],\n",
       "       [0.12467532, 0.87532468],\n",
       "       [0.95549738, 0.04450262],\n",
       "       [0.05343511, 0.94656489],\n",
       "       [0.98969072, 0.01030928],\n",
       "       [0.99742268, 0.00257732],\n",
       "       [0.99206349, 0.00793651],\n",
       "       [0.        , 1.        ],\n",
       "       [0.94736842, 0.05263158],\n",
       "       [0.02879581, 0.97120419],\n",
       "       [0.05851064, 0.94148936],\n",
       "       [0.06162465, 0.93837535],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.99455041, 0.00544959],\n",
       "       [0.        , 1.        ],\n",
       "       [0.93820225, 0.06179775],\n",
       "       [0.07591623, 0.92408377],\n",
       "       [0.99204244, 0.00795756],\n",
       "       [0.17066667, 0.82933333],\n",
       "       [0.00767263, 0.99232737],\n",
       "       [0.05289673, 0.94710327],\n",
       "       [0.00255754, 0.99744246],\n",
       "       [0.82291667, 0.17708333],\n",
       "       [0.08762887, 0.91237113],\n",
       "       [0.09137056, 0.90862944],\n",
       "       [1.        , 0.        ],\n",
       "       [0.944     , 0.056     ],\n",
       "       [0.18346253, 0.81653747],\n",
       "       [0.93994778, 0.06005222],\n",
       "       [0.05978261, 0.94021739],\n",
       "       [0.10242588, 0.89757412],\n",
       "       [0.98963731, 0.01036269],\n",
       "       [0.92746114, 0.07253886],\n",
       "       [0.53543307, 0.46456693],\n",
       "       [0.84293194, 0.15706806],\n",
       "       [1.        , 0.        ],\n",
       "       [0.02710027, 0.97289973],\n",
       "       [0.9609375 , 0.0390625 ],\n",
       "       [0.03341902, 0.96658098],\n",
       "       [0.11375661, 0.88624339],\n",
       "       [0.92307692, 0.07692308],\n",
       "       [1.        , 0.        ],\n",
       "       [0.06878307, 0.93121693],\n",
       "       [0.72236504, 0.27763496]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# oob 샘플에 대한 결정 함수의 값 확인\n",
    "# 결정 함수는 각 훈련 샘플의 클래스 확률을 반환한다 (기반이 되는 예측기가 predict_proba() 메서드를 가지고 있기 때문)\n",
    "# 음성 클래스에 속할 확률, 양성 클래스에 속할 확률\n",
    "bag_clf.oob_decision_function_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.3 랜덤 패치와 랜덤 서브스페이스"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BaggingClassifier는 특성 샘플링도 지원한다. 샘플링은 `max_features`, `bootstrap_features` 두 매개변수로 조절된다. 특성 샘플링은 더 다양한 예측기를 만들며 편향을 늘리는 대신 분산을 낮춘다.\n",
    "\n",
    "이 기법은 (이미지와 같은) 매우 고차원의 데이터셋을 다룰 때 유용하다.\n",
    "\n",
    "* 랜덤 패치 방식<sup>random patches method</sup>\n",
    " * 훈련 특성과 샘플을 모두 샘플링한다.\n",
    "* 랜덤 서브스페이스 방식<sup>random subspaces method</sup>\n",
    " * 훈련 샘플은 모두 사용 (bootstrap = False && max_samples = 1.0)\n",
    " * 특성은 샘플링 (bootstrap_features = True || max_features < 1.0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.4 랜덤 포레스트"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 랜덤 포레스트\n",
    " * 일반적으로 배깅(혹은 페이스팅)을 적용한 결정 트리의 앙상블\n",
    " * 전형적으로 max_samples를 훈련 세트의 크기로 지정한다\n",
    " * `DecisionTreeClassifier` 대신 결정 트리에 최적화되어 사용하기 편리한 `RandomForestClassifier` 사용 가능\n",
    " * 마찬가지로 회귀에는 `RandomForestRegressor` 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RandomForestClassifier는 몇 가지 예외가 존재하나 \n",
    "# (트리 성장을 조절하기 위한) DecisionTreeClassifier 매개변수와 \n",
    "# (앙상블 자체를 제어하는데 필요한) BaggingClassifier 매개변수 모두 가지고 있다\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rnd_clf = RandomForestClassifier(\n",
    "    n_estimators = 500, # 500개의 트리\n",
    "    max_leaf_nodes = 16, # 최대 16개의 리프 노드\n",
    "    n_jobs = -1)\n",
    "rnd_clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred_rf = rnd_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 트리의 노드를 분할할 때 전체 특성 중에서 최선의 특성을 찾는 대신 \n",
    "# 무작위로 선택한 특성 후보 중에서 최적의 특성을 찾는 식으로 무작위성을 더 주입한다\n",
    "# 이는 트리를 더욱 다양하게 만들고 편향을 손해보는 대신 분산을 낮추어 전체적으로 더 훌륭한 모델을 만든다.\n",
    "\n",
    "# BaggingClassifier로 RandomForestClassifier 유사하게 만듦:\n",
    "bag_clf = BaggingClassifier(\n",
    "    DecisionTreeClassifier(max_features = \"auto\", max_leaf_nodes = 16),\n",
    "    n_estimators = 500, max_samples = 1.0, bootstrap = True, n_jobs = -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.4.1 엑스트라 트리"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 엑스트라 트리 <sup>extra-trees</sup>\n",
    "* 익스트림 랜덤 트리 <sup>extremely randomized trees</sup>\n",
    "* 극단적으로 무작위한 트리의 랜덤 포레스트\n",
    " * 최적의 임곗값을 찾는 대신 후보 특성을 이용해 무작위로 분할한 다음 그중에서 최상의 분할 선택\n",
    "* 분류에는 `ExtraTreesClassifier`, 회귀에는 `ExtraTreesRegressor` 사용\n",
    "* 일반적으로 랜덤 포레스트, 엑스트라 트리 둘다 시도해보고 교차 검증으로 비교해보는 것이 유일한 방법\n",
    " * 그리드 탐색으로 하이퍼파라미터 튜닝을 한다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.4.2 특성 중요도"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "랜덤 포레스트의 또 다른 장점은 특성의 상대적 중요도를 측정하기 쉽다는 것.\n",
    "사이킷런은 어떤 특성을 사용한 노드가 (모든 트리에 걸쳐) 평균적으로 불순도를 얼마나 감소시키는지 확인하여 특성의 중요도를 측정한다. 가중치 평균이며 각 노드의 가중치는 연관된 훈련 샘플 수와 같다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sepal length (cm) 0.09785640024079285\n",
      "sepal width (cm) 0.020992419525793487\n",
      "petal length (cm) 0.4144898771668969\n",
      "petal width (cm) 0.4666613030665168\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "\n",
    "iris = load_iris()\n",
    "rnd_clf = RandomForestClassifier( n_estimators = 500, n_jobs = -1 )\n",
    "rnd_clf.fit(iris[\"data\"], iris[\"target\"])\n",
    "\n",
    "# feature_importances_: 사이킷런은 훈련이 끝난 뒤 특성마다 자동으로 이 점수를 계산하고 중요도의 전체 합이 1이 되도록 결괏값을 정규화한다.\n",
    "# iris 데이터셋 특성 별 중요도 출력\n",
    "for name, score in zip(iris[\"feature_names\"], rnd_clf.feature_importances_):\n",
    "    print(name, score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.5 부스팅"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 부스팅<sup>boosting</sup>\n",
    " * 가설 부스팅<sup>hypothesis boosting</sup>\n",
    " * 약한 학습기를 여러 개 연결하여 강한 학습기를 만드는 앙상블 방법\n",
    " * 가장 인기 있는 것은 **에이다부스트**<sup>AdaBoost(adaptive boosting)</sup>와 **그레이디언트 부스팅**<sup>gradient boosting</sup>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.5.1 에이다부스트"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 이전 모델이 과소적합했던(잘못 분류된) 훈련 샘플의 가중치를 더 높이는 것\n",
    "* 샘플의 가중치를 업데이트하면서 순차적으로 학습\n",
    "* 연속된 학습 기법에는 중요한 단점이 있다. 확장성이 높지 않다는 것.\n",
    " * 각 예측기는 이전 예측기가 훈련되고 평가된 후에 학습될 수 있기 때문에 병렬화(혹은 분할)를 할 수 없다.\n",
    "* 분류에는 `AdaBoostClassifier`, 회귀에는 `AdaBoostRegressor` 사용\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=1),\n",
       "                   learning_rate=0.5, n_estimators=200)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "# 클래스가 두 개뿐일 때 SAMME가 에이다부스트와 동일 (예측기 가중치 식 동일)\n",
    "# 예측기가 클래스 확률을 추정할 수 있다면 SAMME.R (Real) 이라는 변종 사용\n",
    "# SAMME.R은 예측값 대신 클래스 확률에 기반하며 일반적으로 성능이 더 좋다\n",
    "# 에이다부스트 앙상블이 훈련 세트에 과대적합되면 추정기 수를 줄이거나 추정기의 규제를 더 강하게 한다\n",
    "ada_clf = AdaBoostClassifier(\n",
    "    DecisionTreeClassifier(max_depth = 1), # 결정 노드 1개, 리프 노드 2개\n",
    "    n_estimators = 200, # 200개의 결정 트리\n",
    "    algorithm = \"SAMME.R\", # 에이다부스트 다중클래스\n",
    "    learning_rate = 0.5)\n",
    "ada_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.5.2 그레이디언트 부스팅"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 에이다부스트처럼 앙상블에 이전까지의 오차를 보정하도록 예측기를 순차적으로 추가한다\n",
    "* 그러나 반복마다 샘플의 가중치를 수정하는 대신 이전 예측기가 만든 **잔여 오차**<sup>residual error</sup>에 새로운 예측기를 학습시킨다\n",
    "\n",
    "#### 그레이디언트 트리 부스팅 <sup>gradient tree boosting</sup>\n",
    "* 그레이디언트 부스티드 회귀 트리<sup>gradieent boosted regression tree (GRBT)</sup>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "X = np.random.rand(100, 1) - 0.5\n",
    "y = 3*X[:, 0]**2 + 0.05 * np.random.randn(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(max_depth=2, random_state=42)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# 잡음이 섞인 2차 곡선 형태의 훈련 세트\n",
    "tree_reg1 = DecisionTreeRegressor(max_depth = 2, random_state=42)\n",
    "tree_reg1.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(max_depth=2, random_state=42)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y2 = y - tree_reg1.predict(X) # 잔여 오차\n",
    "tree_reg2 = DecisionTreeRegressor(max_depth = 2, random_state=42)\n",
    "tree_reg2.fit(X, y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(max_depth=2, random_state=42)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y3 = y2 - tree_reg2.predict(X) # 잔여 오차\n",
    "tree_reg3 = DecisionTreeRegressor(max_depth = 2, random_state=42)\n",
    "tree_reg3.fit(X, y3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모든 트리의 예측을 더하여 새로운 샘플에 대한 예측을 만듦 (앙상블 모델)\n",
    "X_new = np.array([[0.8]])\n",
    "y_pred = sum(tree.predict(X_new) for tree in (tree_reg1, tree_reg2, tree_reg3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.75026781])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingRegressor(learning_rate=1.0, max_depth=2, n_estimators=3)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "# RandomForestRegressor 와 비슷하게 앙상블 훈련 제어하는 매개변수 존재: max_depth, min_samples_leaf\n",
    "# learning_rate 각 트리의 기여 정도를 조절\n",
    "# 축소(shrinkage) \n",
    "# learning_rate = 0.1 낮게 설정하면 앙상블을 훈련 세트에 학습 시키기 위해 많은 트리가 필요하다\n",
    "# but 일반적으로 예측의 성능은 좋아진다\n",
    "gbrt = GradientBoostingRegressor(max_depth = 2, n_estimators = 3, learning_rate = 1.0)\n",
    "gbrt.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingRegressor(max_depth=2, n_estimators=108)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 많은 수의 트리를 먼저 훈련 시키고 최적의 수를 찾는 방법\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y)\n",
    "\n",
    "gbrt = GradientBoostingRegressor(max_depth = 2, n_estimators = 120)\n",
    "gbrt.fit(X_train, y_train)\n",
    "\n",
    "# 최적의 트리 수를 찾기 위한 조기 종료 기법 (4장 참조)\n",
    "# 훈련의 각 단계에서 앙상블에 의해 만들어진 예측기를 순회하는 반복자 반환\n",
    "errors = [mean_squared_error(y_val, y_pred)\n",
    "             for y_pred in gbrt.staged_predict(X_val)]\n",
    "bst_n_estimators = np.argmin(errors) + 1 # 최적의 트리 수\n",
    "\n",
    "gbrt_best = GradientBoostingRegressor(max_depth = 2, n_estimators = bst_n_estimators)\n",
    "gbrt_best.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 실제로 훈련을 중지하는 방법\n",
    "\n",
    "# warmstart = True 사이킷런이 fit() 메서드가 호출될 때 기존 트리를 유지하고 훈련을 추가할 수 있도록 함\n",
    "gbrt = GradientBoostingRegressor(max_depth = 2, warm_start = True)\n",
    "\n",
    "min_val_error = float(\"inf\")\n",
    "error_going_up = 0\n",
    "\n",
    "# 연속해서 5번의 반복 동안 검증 오차가 향상되지 않으면 훈련을 멈춤\n",
    "for n_estimators in range(1, 120):\n",
    "    gbrt.n_estimators = n_estimators\n",
    "    gbrt.fit(X_train, y_train)\n",
    "    y_pred = gbrt.predict(X_val)\n",
    "    val_error = mean_squared_error(y_val, y_pred)\n",
    "    if val_error < min_val_error:\n",
    "        min_val_error = val_error\n",
    "        error_going_up = 0\n",
    "    else:\n",
    "        error_going_up += 1\n",
    "        if error_going_up == 5:\n",
    "            break # 조기 종료"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 확률적 그레이디언트 부스팅 <sup>stochastic gradient boosting</sup>\n",
    "* `subsample` 매개변수로 각 트리가 훈련할 때 사용할 훈련 샘플의 비율을 지정\n",
    " * subsample = 0.25 면 각 트리는 무작위로 선택된 25% 훈련 샘플로 학습\n",
    "* 편향이 높아지는 대신 분산이 낮아진다\n",
    "* 훈련 속도를 상당히 높인다\n",
    "\n",
    "#### XGBoost\n",
    "* 익스트림 그레이디언트 부스팅 <sup>extreme gradient boosting</sup>\n",
    "* 최적화된 그레이디언트 부스팅 구현으로 유명한 파이썬 라이브러리\n",
    "* 패키지 목표는 매우 빠른 속도, 확장성, 이식성\n",
    "* 자동 조기 종료와 같은 여러 좋은 기능도 제공"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting xgboost\n",
      "  Downloading xgboost-1.3.3-py3-none-macosx_10_14_x86_64.macosx_10_15_x86_64.macosx_11_0_x86_64.whl (1.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.2 MB 2.1 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.8/site-packages (from xgboost) (1.18.5)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.8/site-packages (from xgboost) (1.1.0)\n",
      "Installing collected packages: xgboost\n",
      "Successfully installed xgboost-1.3.3\n"
     ]
    }
   ],
   "source": [
    "! pip3 install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "XGBoostError",
     "evalue": "XGBoost Library (libxgboost.dylib) could not be loaded.\nLikely causes:\n  * OpenMP runtime is not installed (vcomp140.dll or libgomp-1.dll for Windows, libomp.dylib for Mac OSX, libgomp.so for Linux and other UNIX-like OSes). Mac OSX users: Run `brew install libomp` to install OpenMP runtime.\n  * You are running 32-bit Python on a 64-bit OS\nError message(s): ['dlopen(/usr/local/lib/python3.8/site-packages/xgboost/lib/libxgboost.dylib, 6): Library not loaded: /usr/local/opt/libomp/lib/libomp.dylib\\n  Referenced from: /usr/local/lib/python3.8/site-packages/xgboost/lib/libxgboost.dylib\\n  Reason: image not found']\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mXGBoostError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-5e4aaaf9815e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mxgboost\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mxgb_reg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxgboost\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mXGBRegressor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mxgb_reg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxgb_reg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/xgboost/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDMatrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDeviceQuantileDMatrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBooster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrabit\u001b[0m  \u001b[0;31m# noqa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/xgboost/core.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[0;31m# load the XGBoost library globally\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m \u001b[0m_LIB\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_load_lib\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/xgboost/core.py\u001b[0m in \u001b[0;36m_load_lib\u001b[0;34m()\u001b[0m\n\u001b[1;32m    155\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mlib_success\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m         \u001b[0mlibname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlib_paths\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m         raise XGBoostError(\n\u001b[0m\u001b[1;32m    158\u001b[0m             \u001b[0;34m'XGBoost Library ({}) could not be loaded.\\n'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlibname\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m             \u001b[0;34m'Likely causes:\\n'\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mXGBoostError\u001b[0m: XGBoost Library (libxgboost.dylib) could not be loaded.\nLikely causes:\n  * OpenMP runtime is not installed (vcomp140.dll or libgomp-1.dll for Windows, libomp.dylib for Mac OSX, libgomp.so for Linux and other UNIX-like OSes). Mac OSX users: Run `brew install libomp` to install OpenMP runtime.\n  * You are running 32-bit Python on a 64-bit OS\nError message(s): ['dlopen(/usr/local/lib/python3.8/site-packages/xgboost/lib/libxgboost.dylib, 6): Library not loaded: /usr/local/opt/libomp/lib/libomp.dylib\\n  Referenced from: /usr/local/lib/python3.8/site-packages/xgboost/lib/libxgboost.dylib\\n  Reason: image not found']\n"
     ]
    }
   ],
   "source": [
    "import xgboost\n",
    "\n",
    "xgb_reg = xgboost.XGBRegressor()\n",
    "xgb_reg.fit(X_train, y_train)\n",
    "y_pred = xgb_reg.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'xbg_reg' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-060895964a0e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m xbg_reg.fit(X_train, y_train,\n\u001b[0m\u001b[1;32m      2\u001b[0m             eval_set=[(X_val, y_val)], early_stopping_rounds = 2)\n\u001b[1;32m      3\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxbg_reg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'xbg_reg' is not defined"
     ]
    }
   ],
   "source": [
    "xbg_reg.fit(X_train, y_train,\n",
    "            eval_set=[(X_val, y_val)], \n",
    "            early_stopping_rounds = 2) # 자동 조기 종료\n",
    "y_pred = xbg_reg.predict(X_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.6 스태킹"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 스태킹<sup>stacking(stacked generalization)</sup>\n",
    " * 앙상블에 속한 모든 예측기의 예측을 취합하는 간단한 함수(e.g. 직접 투표) 대신 취합하는 모델을 훈련시킨다\n",
    " * 예측기들은 각각 다른 값을 예측하고 마지막 예측기(**블렌더**<sup>blender</sup> 혹은 **메타 학습기**<sup>meta learner</sup>)가 각 예측 값을 입력으로 받아 최종 예측을 만듦\n",
    " * 블렌더를 학습시키는 일반적인 방법은 홀드 아웃<sup>hold-out</sup> 세트를 사용하는 것\n",
    " * 사이킷런은 스태킹을 직접 지원하지 않으므로 직접 구현하거나 DESlib 같은 오픈 소스를 사용한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
